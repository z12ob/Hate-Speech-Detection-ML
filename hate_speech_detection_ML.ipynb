{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f6b746",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e624c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd36e01",
   "metadata": {},
   "source": [
    "Import the Dataset\n",
    "\n",
    "**Note: Make sure to run the imports cell above first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(\"twitter.csv\", index_col=0)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    print(\"Columns:\", data.columns.tolist())\n",
    "    print(\"Shape:\", data.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: twitter.csv not found\")\n",
    "    exit(1)\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Failed to parse twitter.csv\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f28234",
   "metadata": {},
   "source": [
    "Load and Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\\n\", data.isnull().sum())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0373849",
   "metadata": {},
   "source": [
    "Map Columns for Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974bc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(df, column='class'):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Error: {column} column not found\")\n",
    "        exit(1)\n",
    "    df['labels'] = df[column].map({0: \"Hate Speech\", 1: \"Offensive Language\", 2: \"Normal\"})\n",
    "    return df\n",
    "\n",
    "data = map_labels(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abb9c9",
   "metadata": {},
   "source": [
    "Apply Mapping and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f294a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[['tweet', 'labels']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb09748",
   "metadata": {},
   "source": [
    "Select Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tweet' not in data.columns or 'labels' not in data.columns:\n",
    "    print(\"Error: Required columns missing\")\n",
    "    exit(1)\n",
    "data = data[['tweet', 'labels']]\n",
    "print(\"Selected data shape:\", data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf6da6",
   "metadata": {},
   "source": [
    "Clean the Sentence in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f68d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    text = \" \".join(text)\n",
    "    return text if text.strip() else \"empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c24821",
   "metadata": {},
   "source": [
    "Stemming and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b317d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not nltk.data.find('corpora/stopwords'):\n",
    "    nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data['tweet'] = data['tweet'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90e051",
   "metadata": {},
   "source": [
    "Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34df20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data['tweet'])\n",
    "y = np.array(data['labels'])\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441cb65",
   "metadata": {},
   "source": [
    "Vectorization and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(clf, 'hate_speech_model.pkl')\n",
    "joblib.dump(cv, 'vectorizer.pkl')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573991c4",
   "metadata": {},
   "source": [
    "Validate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(\"Sample predictions:\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a925f",
   "metadata": {},
   "source": [
    "Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(text, vectorizer, model):\n",
    "    cleaned = clean(text)\n",
    "    if cleaned == \"empty\":\n",
    "        return \"Error: Invalid input after cleaning\"\n",
    "    vectorized = vectorizer.transform([cleaned]).toarray()\n",
    "    return model.predict(vectorized)[0]\n",
    "\n",
    "sample = \"kill\"\n",
    "print(\"Prediction for '{}': {}\".format(sample, predict_sample(sample, cv, clf)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
